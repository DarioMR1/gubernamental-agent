# LangGraph + FastAPI: Estructura de Proyecto Real

Esta guÃ­a explica la estructura pragmÃ¡tica de proyectos LangGraph con FastAPI basada en implementaciones reales de 2024-2025.

## ğŸ“ Estructura General

```
langgraph-fastapi-app/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                    # ğŸš€ FastAPI app entry point
â”‚   â”œâ”€â”€ dependencies.py            # âš¡ Workflow precompilation & DI
â”‚   â”œâ”€â”€ config.py                  # âš™ï¸ Application settings
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                       # ğŸŒ FastAPI HTTP layer
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ agents.py          # ğŸ¤– Agent interaction endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py            # ğŸ’¬ Chat/conversation endpoints
â”‚   â”‚   â”‚   â””â”€â”€ health.py          # â¤ï¸ Health check endpoints
â”‚   â”‚   â””â”€â”€ schemas/               # ğŸ“‹ Pydantic request/response models
â”‚   â”‚       â”œâ”€â”€ requests.py
â”‚   â”‚       â””â”€â”€ responses.py
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/                    # ğŸ§  LangGraph AI layer
â”‚   â”‚   â”œâ”€â”€ workflows/             # ğŸ”„ Graph definitions & orchestration
â”‚   â”‚   â”‚   â”œâ”€â”€ rag_agent.py       # ğŸ“š RAG workflow
â”‚   â”‚   â”‚   â””â”€â”€ chat_agent.py      # ğŸ’­ Chat workflow
â”‚   â”‚   â”œâ”€â”€ nodes/                 # ğŸ”— Individual graph nodes
â”‚   â”‚   â”‚   â”œâ”€â”€ retrieval.py       # ğŸ” Document retrieval
â”‚   â”‚   â”‚   â”œâ”€â”€ generation.py      # âœ¨ LLM text generation
â”‚   â”‚   â”‚   â””â”€â”€ decision.py        # ğŸ¤” Decision/routing logic
â”‚   â”‚   â”œâ”€â”€ tools/                 # ğŸ› ï¸ Agent tools & functions
â”‚   â”‚   â””â”€â”€ prompts/               # ğŸ“ Prompt templates
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                      # ğŸ’¾ Data persistence layer
â”‚   â”‚   â”œâ”€â”€ models.py              # ğŸ—„ï¸ Database/ORM models
â”‚   â”‚   â”œâ”€â”€ database.py            # ğŸ”Œ Database connections
â”‚   â”‚   â””â”€â”€ repositories.py        # ğŸ“Š Data access patterns
â”‚   â”‚
â”‚   â””â”€â”€ utils/                     # ğŸ”§ Shared utilities
â”‚       â”œâ”€â”€ logging.py             # ğŸ“ Logging configuration
â”‚       â”œâ”€â”€ security.py            # ğŸ”’ Authentication helpers
â”‚       â””â”€â”€ helpers.py             # ğŸ¯ Common utilities
â”‚
â”œâ”€â”€ tests/                         # âœ… Test suite
â”œâ”€â”€ docker/                        # ğŸ³ Containerization
â”œâ”€â”€ .env.example                   # ğŸ”‘ Environment template
â””â”€â”€ requirements.txt               # ğŸ“¦ Python dependencies
```

---

## ğŸš€ Archivo Principal: `main.py`

**PropÃ³sito:** Entry point de la aplicaciÃ³n FastAPI

```python
from fastapi import FastAPI, Depends
from src.api.routes import agents, chat, health
from src.dependencies import get_compiled_workflows
from src.config import settings

app = FastAPI(title="LangGraph Agent API")

# Include routers
app.include_router(health.router, prefix="/health", tags=["health"])
app.include_router(agents.router, prefix="/api/v1/agents", tags=["agents"])
app.include_router(chat.router, prefix="/api/v1/chat", tags=["chat"])

@app.on_event("startup")
async def startup_event():
    """Precompile workflows on startup for performance"""
    await get_compiled_workflows()
```

**ConexiÃ³n LangGraph:** Inicializa y precompila workflows durante el startup para optimizar performance.

---

## âš¡ PrecompilaciÃ³n: `dependencies.py`

**PropÃ³sito:** Dependency Injection y precompilaciÃ³n de workflows LangGraph

```python
from functools import lru_cache
from langgraph.graph import StateGraph
from src.agents.workflows.rag_agent import create_rag_workflow
from src.agents.workflows.chat_agent import create_chat_workflow

# Global workflow storage
_compiled_workflows = {}

@lru_cache()
def get_compiled_workflows():
    """Precompile LangGraph workflows for reuse"""
    global _compiled_workflows
    
    if not _compiled_workflows:
        _compiled_workflows = {
            "rag": create_rag_workflow().compile(),
            "chat": create_chat_workflow().compile()
        }
    
    return _compiled_workflows

def get_rag_workflow():
    """Dependency for RAG workflow"""
    workflows = get_compiled_workflows()
    return workflows["rag"]

def get_chat_workflow():
    """Dependency for chat workflow"""
    workflows = get_compiled_workflows()
    return workflows["chat"]
```

**ConexiÃ³n LangGraph:** 
- Precompila workflows en memoria (evita recompilaciÃ³n en cada request)
- Usa `@lru_cache()` para singleton pattern
- Workflows compilados son reutilizables y thread-safe

---

## âš™ï¸ ConfiguraciÃ³n: `config.py`

**PropÃ³sito:** ConfiguraciÃ³n centralizada de la aplicaciÃ³n

```python
from pydantic import BaseSettings

class Settings(BaseSettings):
    # API Configuration
    app_name: str = "LangGraph Agent API"
    debug: bool = False
    
    # LLM Configuration
    openai_api_key: str
    model_name: str = "gpt-4"
    temperature: float = 0.7
    
    # Vector Store
    chroma_persist_directory: str = "./chroma_db"
    
    # Database
    database_url: str = "sqlite:///./app.db"
    
    class Config:
        env_file = ".env"

settings = Settings()
```

**ConexiÃ³n LangGraph:** Proporciona configuraciÃ³n para modelos LLM, bases de datos vectoriales y checkpoints.

---

## ğŸŒ API Layer: Routes

### `api/routes/agents.py`

**PropÃ³sito:** Endpoints para interactuar con agentes LangGraph

```python
from fastapi import APIRouter, Depends, HTTPException
from src.dependencies import get_rag_workflow
from src.api.schemas.requests import AgentRequest
from src.api.schemas.responses import AgentResponse

router = APIRouter()

@router.post("/invoke", response_model=AgentResponse)
async def invoke_agent(
    request: AgentRequest,
    workflow = Depends(get_rag_workflow)
):
    """Invoke RAG agent with user query"""
    try:
        result = await workflow.ainvoke({
            "question": request.question,
            "context": request.context or []
        })
        
        return AgentResponse(
            answer=result["answer"],
            sources=result.get("sources", []),
            metadata=result.get("metadata", {})
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/stream")
async def stream_agent(
    request: AgentRequest,
    workflow = Depends(get_rag_workflow)
):
    """Stream agent responses"""
    async def generate():
        async for chunk in workflow.astream({
            "question": request.question
        }):
            yield f"data: {chunk}\n\n"
    
    return StreamingResponse(generate(), media_type="text/plain")
```

**ConexiÃ³n LangGraph:**
- Usa `workflow.ainvoke()` para ejecuciÃ³n asÃ­ncrona
- `workflow.astream()` para streaming responses
- Dependency injection de workflows precompilados

### `api/routes/chat.py`

**PropÃ³sito:** Endpoints para conversaciones con memoria

```python
@router.post("/conversations/{thread_id}")
async def continue_conversation(
    thread_id: str,
    request: ChatRequest,
    workflow = Depends(get_chat_workflow)
):
    """Continue conversation with thread memory"""
    config = {"configurable": {"thread_id": thread_id}}
    
    result = await workflow.ainvoke(
        {"message": request.message},
        config=config
    )
    
    return ChatResponse(
        response=result["response"],
        thread_id=thread_id
    )
```

**ConexiÃ³n LangGraph:**
- Usa `thread_id` para manejo de memoria conversacional
- LangGraph maneja automÃ¡ticamente checkpoints por thread

---

## ğŸ§  LangGraph Layer: Agents

### `agents/workflows/rag_agent.py`

**PropÃ³sito:** DefiniciÃ³n del workflow RAG usando LangGraph

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict, List
from src.agents.nodes.retrieval import retrieve_documents
from src.agents.nodes.generation import generate_answer
from src.agents.nodes.decision import should_retrieve

class RAGState(TypedDict):
    question: str
    documents: List[str]
    answer: str
    needs_retrieval: bool

def create_rag_workflow():
    """Create RAG workflow graph"""
    workflow = StateGraph(RAGState)
    
    # Add nodes
    workflow.add_node("decide", should_retrieve)
    workflow.add_node("retrieve", retrieve_documents)
    workflow.add_node("generate", generate_answer)
    
    # Add edges
    workflow.set_entry_point("decide")
    workflow.add_conditional_edges(
        "decide",
        lambda x: "retrieve" if x["needs_retrieval"] else "generate"
    )
    workflow.add_edge("retrieve", "generate")
    workflow.add_edge("generate", END)
    
    return workflow
```

**ConexiÃ³n LangGraph:**
- `StateGraph` define el flujo de estados
- Cada node es una funciÃ³n que transforma el estado
- Conditional edges permiten routing dinÃ¡mico

### `agents/nodes/retrieval.py`

**PropÃ³sito:** Nodo para recuperaciÃ³n de documentos

```python
from src.data.repositories import get_vector_store
from src.agents.nodes.base import BaseNode

async def retrieve_documents(state: dict) -> dict:
    """Retrieve relevant documents for question"""
    question = state["question"]
    
    # Get vector store
    vector_store = get_vector_store()
    
    # Retrieve similar documents
    docs = await vector_store.asimilarity_search(
        question, 
        k=5
    )
    
    # Update state
    state["documents"] = [doc.page_content for doc in docs]
    state["needs_retrieval"] = False
    
    return state
```

**ConexiÃ³n LangGraph:**
- FunciÃ³n que recibe y retorna el estado
- Modifica el estado compartido del workflow
- Integra con vector stores para RAG

### `agents/nodes/generation.py`

**PropÃ³sito:** Nodo para generaciÃ³n de respuestas con LLM

```python
from langchain.chat_models import ChatOpenAI
from src.agents.prompts.rag_prompts import RAG_PROMPT
from src.config import settings

llm = ChatOpenAI(
    model=settings.model_name,
    temperature=settings.temperature
)

async def generate_answer(state: dict) -> dict:
    """Generate answer using LLM"""
    question = state["question"]
    documents = state.get("documents", [])
    
    # Format context
    context = "\n".join(documents)
    
    # Generate response
    prompt = RAG_PROMPT.format(
        context=context,
        question=question
    )
    
    response = await llm.ainvoke(prompt)
    
    # Update state
    state["answer"] = response.content
    
    return state
```

**ConexiÃ³n LangGraph:**
- Node function que integra LLM
- Usa estado compartido para context
- Retorna estado actualizado

---

## ğŸ“ Prompts: `agents/prompts/rag_prompts.py`

**PropÃ³sito:** Templates de prompts reutilizables

```python
RAG_PROMPT = """
BasÃ¡ndote en el siguiente contexto, responde la pregunta del usuario.

Contexto:
{context}

Pregunta: {question}

Respuesta:
"""

CHAT_PROMPT = """
Eres un asistente Ãºtil. Responde de manera conversacional.

Historial:
{history}

Usuario: {message}
Asistente:
"""
```

**ConexiÃ³n LangGraph:** Prompts reutilizables en mÃºltiples nodes y workflows.

---

## ğŸ’¾ Data Layer

### `data/database.py`

**PropÃ³sito:** Conexiones a bases de datos y vector stores

```python
from chromadb import Client
from sqlalchemy import create_engine
from src.config import settings

# Vector store connection
def get_vector_store():
    client = Client()
    collection = client.get_or_create_collection("documents")
    return collection

# SQL database connection
engine = create_engine(settings.database_url)
```

**ConexiÃ³n LangGraph:**
- Vector stores para RAG retrieval
- SQL database para checkpoints persistentes
- Memoria conversacional

---

## ğŸ”„ Flujo Completo de EjecuciÃ³n

### 1. **Startup**
```
main.py â†’ dependencies.py â†’ workflows precompilados
```

### 2. **Request Handling**
```
FastAPI endpoint â†’ Dependency injection â†’ Workflow execution
```

### 3. **LangGraph Execution**
```
StateGraph â†’ Node functions â†’ State transformations â†’ Response
```

### 4. **Memory Management**
```
Thread ID â†’ LangGraph checkpoints â†’ Persistent conversation
```

---

## ğŸ¯ Patrones Clave LangGraph

### **State Management**
- Estado compartido entre nodes
- TypedDict para type safety
- Immutable transformations

### **Workflow Compilation**
- PrecompilaciÃ³n para performance
- Thread-safe execution
- ReutilizaciÃ³n de workflows

### **Memory & Checkpoints**
- Thread-based conversations
- Automatic state persistence
- Resume from any point

### **Streaming Responses**
- Real-time output
- Progressive generation
- Better UX

---

## ğŸš€ Beneficios de esta Estructura

âœ… **Performance:** Workflows precompilados  
âœ… **Escalabilidad:** Stateless design  
âœ… **Mantenibilidad:** SeparaciÃ³n clara de responsabilidades  
âœ… **Flexibilidad:** Easy workflow modification  
âœ… **Type Safety:** Pydantic + TypedDict  
âœ… **Observabilidad:** Structured logging  

Esta estructura refleja **patrones reales** de proyectos LangGraph + FastAPI en producciÃ³n, priorizando simplicidad y efectividad sobre arquitectura compleja.